{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Participant Exclusion based on Outlier Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import sem\n",
    "import json\n",
    "import csv\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclusion Criteria\n",
    " \n",
    " Pre-registered on OSF: https://osf.io/fuhpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineCSVs(datafolder):\n",
    "    \"\"\"\n",
    "    Combine all participant data into one pandas df\n",
    "    OR \n",
    "    Create df for single participant file \n",
    "    \"\"\"\n",
    "    \n",
    "    exclude = []\n",
    "    \n",
    "    #checks if path is a file\n",
    "    isFile = os.path.isfile(datafolder)\n",
    "\n",
    "    #checks if path is a directory\n",
    "    \n",
    "    isDirectory = os.path.isdir(datafolder)\n",
    "    \n",
    "    if isDirectory == True:\n",
    "        data = []\n",
    "        for filename in os.listdir(datafolder):\n",
    "            if 'csv' in filename:\n",
    "                path = datafolder + \"/\" + filename\n",
    "                df = pd.read_csv(path, index_col=None, header=0)\n",
    "                if df.experimentName.unique()=='Gabor-Discrimination':\n",
    "                    if df.versionName.unique()=='v2':\n",
    "                        subjID = df.subjID.unique()[0]\n",
    "                        if subjID not in exclude:\n",
    "                            data.append(df)\n",
    "\n",
    "                \n",
    "        input_frame = pd.concat(data, axis=0, ignore_index=True)\n",
    "        \n",
    "    if isFile == True:\n",
    "        if 'csv' in datafolder:\n",
    "            input_frame = pd.read_csv(datafolder, index_col=None, header=0)\n",
    "    \n",
    "    print('Number of participants before cleaning: ', len(input_frame.subjID.unique()))\n",
    "\n",
    " \n",
    "    return input_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path = '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/OSS_Exp2_MTurk/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of participants before cleaning:  294\n"
     ]
    }
   ],
   "source": [
    "input_data = combineCSVs(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanbyPracticeTries(df, num_allowed_tries):\n",
    "    all_subjIDs = df.subjID.unique()\n",
    "    \n",
    "    remove = []\n",
    "    df2_list = []\n",
    "    prac_too_many_dist = []\n",
    "    for subj in all_subjIDs:\n",
    "        count = 0\n",
    "        subj_df = df.loc[df['subjID'] == subj]\n",
    "        cleaned_subj_df = subj_df.copy(deep=True) # prevent setting with copy warning \n",
    "        \n",
    "        subj_num_practice_tries = cleaned_subj_df.pracTries.unique()[0]\n",
    "        \n",
    "        if subj_num_practice_tries > num_allowed_tries:\n",
    "            prac_too_many_dist.append(subj_num_practice_tries)\n",
    "            remove.append(subj)\n",
    "        else:  \n",
    "            df2_list.append(cleaned_subj_df)\n",
    "    \n",
    "    df2 = pd.concat(df2_list)\n",
    "            \n",
    "    print('Number of participants with more than ' + str(num_allowed_tries) + ' practice tries:', len(remove))\n",
    "    \n",
    "    return df2, prac_too_many_dist\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of participants with more than 3 practice tries: 18\n"
     ]
    }
   ],
   "source": [
    "pracTries_cleaned_data, prac_dist = cleanbyPracticeTries(input_data, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pracTries_cleaned_data.subjID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def Accuracy_Cleaning(df, accuracy_threshold, num_trials):\n",
    "    \"\"\"    \n",
    "    Remove participants with overall accuracy below the accuracy threshold (e.g. 0.7)\n",
    "    \n",
    "    Returns:\n",
    "        data of participants that passed the accuracy threshold \n",
    "        list of the number of correct trials each participant got\n",
    "    \"\"\"\n",
    "    #List unique values in the df['subjID'] column\n",
    "    all_subjIDs = df.subjID.unique()\n",
    "    \n",
    "    remove = []\n",
    "    df2_list = []\n",
    "    list_trials_correct = []\n",
    "    # number of participants where exclusion is because all responses were 'none'\n",
    "    c = 0\n",
    "    \n",
    "    for subj in all_subjIDs:\n",
    "        keypresses = []\n",
    "\n",
    "        subj_df = df.loc[df['subjID'] == subj]\n",
    "        cleaned_subj_df = subj_df.copy(deep=True) # prevent setting with copy warning \n",
    "        \n",
    "        subj_num_correct_trials = 0\n",
    "        \n",
    "        acc_column = np.array(list(subj_df['accuracy']))\n",
    "        sum_acc = np.sum(acc_column)\n",
    "        \n",
    "        \n",
    "        for idx, row in subj_df.iterrows():\n",
    "            trial_acc = row['accuracy']\n",
    "            if trial_acc == 1:\n",
    "                subj_num_correct_trials += 1\n",
    "            else:\n",
    "                keypresses.append(row['keyPress'])\n",
    "        \n",
    "        subj_acc = sum_acc/num_trials\n",
    "#         print(acc_column)\n",
    "#         print(subj, sum_acc, subj_acc)\n",
    "                \n",
    "        # minimum number of trials correct the participant must have to be included\n",
    "        if subj_acc < accuracy_threshold:\n",
    "#             print(subj_acc, subj)\n",
    "            remove.append(subj)\n",
    "#             print(len([x for x in keypresses if x == 'none']))\n",
    "            if len([x for x in keypresses if x == 'none']) >= 4:\n",
    "                c += 1\n",
    "            \n",
    "#         else:\n",
    "#             print(subj_acc)\n",
    "        \n",
    "        list_trials_correct.append(subj_num_correct_trials)\n",
    "        \n",
    "        df2_list.append(cleaned_subj_df)\n",
    "    \n",
    "    df2 = pd.concat(df2_list)\n",
    "    \n",
    "    print(\"Number of Participants with accuracy below 70%: \", len(remove))\n",
    "    \n",
    "    for index, row in df2.iterrows():\n",
    "        if row['subjID'] in remove:\n",
    "            df2.drop(index, inplace=True)\n",
    "                \n",
    "    print('Number of participants that did not respond for 4 or more trials:', c)\n",
    "    \n",
    "    print('Number of participants left: ', len(df2.subjID.unique()))\n",
    "                \n",
    "    return df2, list_trials_correct, keypresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Participants with accuracy below 70%:  35\n",
      "Number of participants that did not respond for 4 or more trials: 0\n",
      "Number of participants left:  241\n"
     ]
    }
   ],
   "source": [
    "Accuracy_cleaned_data, correct_trials_distribution, keypresses = Accuracy_Cleaning(pracTries_cleaned_data, 0.7, num_trials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def RT_Cleaning(df, outlier_range, num_trials):\n",
    "    \"\"\"\n",
    "    Remove trials where trial RT is outside of the defined outlier range \n",
    "    \n",
    "    Returns:\n",
    "        dataframe with outlier RT trials removed\n",
    "        list of all RTs \n",
    "    \"\"\"\n",
    "    #List unique values in the df['subjID'] column\n",
    "    all_subjIDs = df.subjID.unique()\n",
    "    print(len(all_subjIDs))\n",
    "    \n",
    "    remove = []\n",
    "    df2_list = []\n",
    "    total_RT_outliers = 0\n",
    "    total = 0\n",
    "    list_trialRT = []\n",
    "    for subj in all_subjIDs:\n",
    "        count = 0\n",
    "        subj_df = df.loc[df['subjID'] == subj]\n",
    "        cleaned_subj_df = subj_df.copy(deep=True) # prevent setting with copy warning \n",
    "\n",
    "        for idx, row in subj_df.iterrows():\n",
    "            total += 1\n",
    "            RT = row[\"RT\"]\n",
    "            list_trialRT.append(RT)\n",
    "            if RT < outlier_range[0]: # outlier\n",
    "                cleaned_subj_df.drop([idx], inplace=True)\n",
    "                count += 1\n",
    "                total_RT_outliers += 1\n",
    "            if RT > outlier_range[1]:\n",
    "                cleaned_subj_df.drop([idx], inplace=True)\n",
    "                count += 1\n",
    "                total_RT_outliers += 1\n",
    "        \n",
    "        df2_list.append(cleaned_subj_df)\n",
    "    \n",
    "    df2 = pd.concat(df2_list)\n",
    "    print(len(df2.subjID.unique()))\n",
    "                \n",
    "    return df2, list_trialRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241\n",
      "240\n"
     ]
    }
   ],
   "source": [
    "RT_cleaned_data, trialRTs_distribution = RT_Cleaning(Accuracy_cleaned_data, [250, 5000], num_trials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def finalTrialCountCheck(df, num_trials, min_trials):\n",
    "    \"\"\"\n",
    "    If more then 10% of a participants data is missing, remove the participant\n",
    "    \"\"\"\n",
    "    #List unique values in the df['subjID'] column\n",
    "    all_subjIDs = df.subjID.unique()\n",
    "    \n",
    "    remove = []\n",
    "    for subj in all_subjIDs:\n",
    "        subj_df = df.loc[df['subjID'] == subj]\n",
    "        count_trials = len(subj_df.index)\n",
    "        if count_trials < min_trials:\n",
    "            remove.append(subj)\n",
    "            \n",
    "#         threshold_trials_remaining = num_trials - math.floor(num_trials * 0.1)\n",
    "\n",
    "#         if count_trials <= threshold_trials_remaining:\n",
    "#             remove.append(subj)\n",
    "            \n",
    "    print(\"Number of Participants with >= 10% trials removed: \", len(remove))\n",
    "            \n",
    "    for subj in remove:\n",
    "        df.drop(df[df['subjID'] == subj].index, inplace = True) \n",
    "                    \n",
    "    print(\"Number of participants left: \",len(df.subjID.unique()))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Participants with >= 10% trials removed:  18\n",
      "Number of participants left:  222\n"
     ]
    }
   ],
   "source": [
    "finalTrialCount_data = finalTrialCountCheck(RT_cleaned_data, num_trials, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_final_data = finalTrialCount_data.copy(deep=True)\n",
    "len(raw_final_data.subjID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jsons/s3_even.json 8\n",
      "jsons/s0_odd.json 10\n",
      "jsons/s8_even.json 10\n",
      "jsons/s1_even.json 10\n",
      "jsons/s4_even.json 8\n",
      "jsons/s8_odd.json 9\n",
      "jsons/s5_odd.json 8\n",
      "jsons/s9_odd.json 11\n",
      "jsons/s11_even.json 8\n",
      "jsons/s4_odd.json 11\n",
      "jsons/s2_odd.json 10\n",
      "jsons/s1_odd.json 9\n",
      "jsons/s2_even.json 8\n",
      "jsons/s3_odd.json 8\n",
      "jsons/s9_even.json 10\n",
      "jsons/s11_odd.json 11\n",
      "jsons/s10_odd.json 9\n",
      "jsons/s5_even.json 9\n",
      "jsons/s6_odd.json 10\n",
      "jsons/s10_even.json 8\n",
      "jsons/s7_even.json 11\n",
      "jsons/s6_even.json 9\n",
      "jsons/s7_odd.json 10\n",
      "jsons/s0_even.json 8\n"
     ]
    }
   ],
   "source": [
    "for seq in raw_final_data.sequenceName.unique():\n",
    "    seq_df = raw_final_data.loc[raw_final_data['sequenceName']==seq]\n",
    "    print(seq, len(seq_df.subjID.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sequence Tracking File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "jsons_dir = '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/OSS_Exp2_MTurk/jsons'\n",
    "\n",
    "file_dest = '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/OSS_Exp2_MTurk/sequence_tracking/'\n",
    "\n",
    "sequence_sampling_dict = {}\n",
    "for seq_name in os.listdir(jsons_dir):\n",
    "    if '.json' in seq_name:\n",
    "        sequence_sampling_dict[seq_name] = []\n",
    "\n",
    "# Convert and write JSON object to file\n",
    "v0_filename = 'b0_GD_master_sequence_tracking.json'\n",
    "with open(file_dest + v0_filename, \"w\") as outfile: \n",
    "    json.dump(sequence_sampling_dict, outfile)\n",
    "\n",
    "print(len(sequence_sampling_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/OSS_Exp2_MTurk/sequence_tracking/b4_GD_master_sequence_tracking.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'s5_even.json': ['738830',\n",
       "  '287865',\n",
       "  '287219',\n",
       "  '268532',\n",
       "  '393692',\n",
       "  '959033',\n",
       "  '887273',\n",
       "  '528857',\n",
       "  '697760'],\n",
       " 's8_even.json': ['589443',\n",
       "  '840050',\n",
       "  '676893',\n",
       "  '102824',\n",
       "  '659411',\n",
       "  '563726',\n",
       "  '232153',\n",
       "  '247187'],\n",
       " 's0_odd.json': ['215484',\n",
       "  '830815',\n",
       "  '775834',\n",
       "  '447827',\n",
       "  '706514',\n",
       "  '308283',\n",
       "  '891679',\n",
       "  '830827',\n",
       "  '569628',\n",
       "  '896945'],\n",
       " 's1_odd.json': ['209897',\n",
       "  '235416',\n",
       "  '533065',\n",
       "  '563135',\n",
       "  '641993',\n",
       "  '568669',\n",
       "  '931030'],\n",
       " 's3_even.json': ['693191',\n",
       "  '630325',\n",
       "  '720363',\n",
       "  '830531',\n",
       "  '466141',\n",
       "  '989662',\n",
       "  '502903',\n",
       "  '932818'],\n",
       " 's11_even.json': ['992898',\n",
       "  '860542',\n",
       "  '395635',\n",
       "  '224395',\n",
       "  '601680',\n",
       "  '171324',\n",
       "  '975033',\n",
       "  '148661'],\n",
       " 's4_even.json': ['712649',\n",
       "  '346283',\n",
       "  '395009',\n",
       "  '227570',\n",
       "  '177433',\n",
       "  '166575',\n",
       "  '462302',\n",
       "  '798676'],\n",
       " 's7_odd.json': ['664257',\n",
       "  '284853',\n",
       "  '623609',\n",
       "  '442290',\n",
       "  '688024',\n",
       "  '709908',\n",
       "  '819964',\n",
       "  '320153',\n",
       "  '770423',\n",
       "  '457056'],\n",
       " 's6_odd.json': ['536270',\n",
       "  '436808',\n",
       "  '585235',\n",
       "  '486712',\n",
       "  '875789',\n",
       "  '527415',\n",
       "  '220688',\n",
       "  '558834',\n",
       "  '619407'],\n",
       " 's9_even.json': ['186776',\n",
       "  '483933',\n",
       "  '926536',\n",
       "  '446162',\n",
       "  '824465',\n",
       "  '119538',\n",
       "  '238345',\n",
       "  '287257',\n",
       "  '719422'],\n",
       " 's2_even.json': ['247843',\n",
       "  '288777',\n",
       "  '355729',\n",
       "  '934908',\n",
       "  '214528',\n",
       "  '557697',\n",
       "  '325895',\n",
       "  '328999'],\n",
       " 's10_even.json': ['768413',\n",
       "  '475227',\n",
       "  '375376',\n",
       "  '770346',\n",
       "  '129279',\n",
       "  '540620',\n",
       "  '397508',\n",
       "  '161372'],\n",
       " 's9_odd.json': ['796808',\n",
       "  '594803',\n",
       "  '421080',\n",
       "  '516796',\n",
       "  '319991',\n",
       "  '857554',\n",
       "  '434645',\n",
       "  '922111',\n",
       "  '515625',\n",
       "  '587428'],\n",
       " 's8_odd.json': ['209396',\n",
       "  '811761',\n",
       "  '140300',\n",
       "  '376176',\n",
       "  '880566',\n",
       "  '265680',\n",
       "  '949611',\n",
       "  '831099',\n",
       "  '702600'],\n",
       " 's7_even.json': ['340960',\n",
       "  '771265',\n",
       "  '670569',\n",
       "  '969117',\n",
       "  '165538',\n",
       "  '727792',\n",
       "  '380944',\n",
       "  '139532',\n",
       "  '228524'],\n",
       " 's3_odd.json': ['110636',\n",
       "  '927939',\n",
       "  '283309',\n",
       "  '850033',\n",
       "  '569268',\n",
       "  '601118',\n",
       "  '619377'],\n",
       " 's2_odd.json': ['698797',\n",
       "  '274316',\n",
       "  '855146',\n",
       "  '495020',\n",
       "  '349807',\n",
       "  '423131',\n",
       "  '342623',\n",
       "  '471406',\n",
       "  '159429'],\n",
       " 's1_even.json': ['253298',\n",
       "  '674279',\n",
       "  '960461',\n",
       "  '224919',\n",
       "  '859449',\n",
       "  '958657',\n",
       "  '378764',\n",
       "  '997250',\n",
       "  '371787'],\n",
       " 's6_even.json': ['122690',\n",
       "  '524119',\n",
       "  '987848',\n",
       "  '541600',\n",
       "  '629220',\n",
       "  '656425',\n",
       "  '935014',\n",
       "  '873075'],\n",
       " 's10_odd.json': ['410237',\n",
       "  '686174',\n",
       "  '774443',\n",
       "  '829158',\n",
       "  '240510',\n",
       "  '684761',\n",
       "  '352249',\n",
       "  '215584',\n",
       "  '575120'],\n",
       " 's11_odd.json': ['457989',\n",
       "  '729209',\n",
       "  '815194',\n",
       "  '940341',\n",
       "  '663907',\n",
       "  '959909',\n",
       "  '986791',\n",
       "  '317739',\n",
       "  '623966',\n",
       "  '906353'],\n",
       " 's0_even.json': ['278662',\n",
       "  '845483',\n",
       "  '106284',\n",
       "  '321184',\n",
       "  '909920',\n",
       "  '561837',\n",
       "  '773700',\n",
       "  '294489'],\n",
       " 's4_odd.json': ['568899',\n",
       "  '166716',\n",
       "  '529629',\n",
       "  '237050',\n",
       "  '483319',\n",
       "  '731908',\n",
       "  '336959',\n",
       "  '477762',\n",
       "  '765771'],\n",
       " 's5_odd.json': ['838604',\n",
       "  '768689',\n",
       "  '847947',\n",
       "  '916589',\n",
       "  '509733',\n",
       "  '980783',\n",
       "  '157330']}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the version for the sequence tracking \n",
    "\n",
    "prev_version = 'b4'\n",
    "new_version = 'b5'\n",
    "\n",
    "# select path for the last previous sequence tracking file \n",
    "\n",
    "sequence_sampling_path = file_dest + prev_version + '_GD_master_sequence_tracking.json'\n",
    "print(sequence_sampling_path)\n",
    "# Opening JSON file\n",
    "f = open(sequence_sampling_path)\n",
    "  \n",
    "# returns JSON object as a dictionary\n",
    "sequence_sampling = json.load(f)\n",
    "\n",
    "sequence_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences previously sampled:  24\n"
     ]
    }
   ],
   "source": [
    "# print number of sequences that have been sampled by the previous batch\n",
    "prev_sampled_count = 0\n",
    "for seq in sequence_sampling:\n",
    "    if len(sequence_sampling[seq]) > 0:\n",
    "        prev_sampled_count += 1\n",
    "print('Number of sequences previously sampled: ', prev_sampled_count)       \n",
    "\n",
    "new_sequence_sampling = sequence_sampling\n",
    "# update sequence sampling dictionary\n",
    "for subj in raw_final_data.subjID.unique():\n",
    "    subj_df = raw_final_data.loc[raw_final_data['subjID'] == subj]\n",
    "    subj_seq = subj_df.sequenceName.unique()[0].split('/')[1]\n",
    "    # add subj to list for its corresponding sequence\n",
    "    new_sequence_sampling[subj_seq].append(str(subj))\n",
    "\n",
    "    \n",
    "sampled_count = 0\n",
    "unsampled_count = 0\n",
    "for seq in new_sequence_sampling:\n",
    "    if len(new_sequence_sampling[seq]) > 0:\n",
    "        # remove duplicates of the same id\n",
    "        new_sequence_sampling[seq] = list(set(new_sequence_sampling[seq]))\n",
    "        sampled_count += 1\n",
    "    else:\n",
    "        unsampled_count += 1\n",
    "        new_sequence_sampling[seq] = []\n",
    "        \n",
    "# print('Number of sequences sampled now: ', sampled_count, '/', len(raw_final_data.sequenceName.unique()))\n",
    "\n",
    "# print('Number of sequences to be sampled: ', unsampled_count)\n",
    "\n",
    "# print('Number sampled + to be sampled = 192: ', sampled_count + unsampled_count==192)\n",
    "\n",
    "\n",
    "with open(file_dest + new_version + \"_GD_master_sequence_tracking.json\", \"w\") as outfile:\n",
    "    json.dump(new_sequence_sampling, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_sequence_sampling.keys()) * 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_to_replace = 0\n",
    "for key in new_sequence_sampling:\n",
    "    if len(new_sequence_sampling[key]) < 8:\n",
    "        print(key, len(new_sequence_sampling[key]))\n",
    "        count_to_replace += (8-len(new_sequence_sampling[key]))\n",
    "\n",
    "count_to_replace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Sequences to Replace\n",
    "\n",
    "In this design, there need to be four participants per sequence (24 total sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goal_participants_per_seq = 8\n",
    "sequences_to_replace = []\n",
    "\n",
    "total = 0\n",
    "for seq_key in new_sequence_sampling:\n",
    "    if len(new_sequence_sampling[seq_key]) < goal_participants_per_seq:\n",
    "        needed = goal_participants_per_seq - len(new_sequence_sampling[seq_key])\n",
    "        total += needed\n",
    "        # print(seq_key, needed)\n",
    "        for i in range(needed):\n",
    "            sequences_to_replace.append(seq_key)\n",
    "\n",
    "len(set(sequences_to_replace)),len(sequences_to_replace), total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new variables file and new counterbalancing file\n",
    "\n",
    "Cannot have multiple rows with the same url in the variables file right now, which is why this is necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Counterbalancing File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "paths = []\n",
    "for json in sequences_to_replace:\n",
    "    paths.append({'Path':'jsons/' + json, 'Sampled': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv header\n",
    "fieldnames = ['Path', 'Sampled']\n",
    "\n",
    "# csv data\n",
    "rows = paths\n",
    "\n",
    "dest = '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/OSS_Exp2_MTurk/'\n",
    "\n",
    "with open(dest + 'counterbalancing.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Sampled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jsons/s1_odd.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jsons/s3_odd.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jsons/s5_odd.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Path  Sampled\n",
       "0  jsons/s1_odd.json        0\n",
       "1  jsons/s3_odd.json        0\n",
       "2  jsons/s5_odd.json        0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counterbalancing_df = pd.read_csv(dest + 'counterbalancing.csv')\n",
    "\n",
    "counterbalancing_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Variables File\n",
    "\n",
    "The row in the counterbalancing csv does NOT match the url fragment since the indexing includes the path row.\n",
    "\n",
    "The url fragment is the counterbalancing df index + 1 --> this has been validated in the console log of the experiment\n",
    "\n",
    "To backtrack from the url fragments to the corresponding row of the counterbalancing csv: row = url_fragment + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_fragments = []\n",
    "for i in range(len(sequences_to_replace)):\n",
    "    url_fragments.append(i + 1)\n",
    "\n",
    "# should be equal\n",
    "len(url_fragments), len(set(url_fragments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number for the NEXT batch \n",
    "batch = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_variables_csv = '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/OSS_Exp2_MTurk/batch_variables/'\n",
    "\n",
    "base_url = 'http://54.235.29.9/FacialAge/OSS_Exp2_MTurk/Gab_OSS_HTML.html#'\n",
    "\n",
    "variables = {'experiment_url': [], 'sampled': []}\n",
    "\n",
    "for fragment in url_fragments:\n",
    "     variables['experiment_url'].append(base_url + str(fragment))\n",
    "     variables['sampled'].append('unsampled')\n",
    "\n",
    "variables_df = pd.DataFrame(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_url</th>\n",
       "      <th>sampled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://54.235.29.9/FacialAge/OSS_Exp2_MTurk/Ga...</td>\n",
       "      <td>unsampled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://54.235.29.9/FacialAge/OSS_Exp2_MTurk/Ga...</td>\n",
       "      <td>unsampled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://54.235.29.9/FacialAge/OSS_Exp2_MTurk/Ga...</td>\n",
       "      <td>unsampled</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      experiment_url    sampled\n",
       "0  http://54.235.29.9/FacialAge/OSS_Exp2_MTurk/Ga...  unsampled\n",
       "1  http://54.235.29.9/FacialAge/OSS_Exp2_MTurk/Ga...  unsampled\n",
       "2  http://54.235.29.9/FacialAge/OSS_Exp2_MTurk/Ga...  unsampled"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_df.to_csv(dest_variables_csv + 'gab-discrim-variables' + '-B' + str(batch) + '.csv', index=False)\n",
    "\n",
    "server_dest = '/Users/prachimahableshwarkar/Documents/GW/spatial_perception/app/'\n",
    "\n",
    "variables_df.to_csv(server_dest + 'variables.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
